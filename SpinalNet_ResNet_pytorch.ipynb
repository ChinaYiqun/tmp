{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpinalNet_ResNet-pytorch",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPkHqpmVc3OzMRua3Co97Yz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChinaYiqun/tmp/blob/main/SpinalNet_ResNet_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Jv5cdctfOoK"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\r\n",
        "tf.disable_v2_behavior()\r\n",
        "!wget https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\r\n",
        "!wget https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat \r\n",
        "!tar zxvf 102flowers.tgz -C ./\r\n",
        "import os\r\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n",
        "import numpy as np\r\n",
        "from scipy.io import loadmat\r\n",
        "data_m= loadmat(\"imagelabels.mat\")\r\n",
        "labels = data_m['labels'].tolist()[0]\r\n",
        "\r\n",
        "import os\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import numpy as np\r\n",
        "from torchvision import datasets, transforms, models\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRZPu7-2fYSi"
      },
      "source": [
        "data_dir = \".\"\r\n",
        "model_name = \"SpinalNet_ResNet\"\r\n",
        "num_classes = 102\r\n",
        "batch_size = 32\r\n",
        "num_epochs = 20\r\n",
        "input_size = 224\r\n",
        "lr = 1e-3\r\n",
        "momentum = 0.9\r\n",
        "is_fixed = True\r\n",
        "use_pretrained = True\r\n",
        "is_train = True\r\n",
        "is_test = True\r\n",
        "\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uz-8Gl3faCi"
      },
      "source": [
        "import shutil\r\n",
        "def mkdir_for_keras(dirname):\r\n",
        "  os.mkdir(dirname)\r\n",
        "  for i in range(1,103):\r\n",
        "    file_name=\"{}_label\".format(i)\r\n",
        "    os.makedirs(dirname + '/' + file_name)\r\n",
        "def move2dir(src,trainfolder,validfoder,labels,split = 0.2):\r\n",
        "  for folderName, subfolders, filenames in os.walk(src):\r\n",
        "    for filename in filenames:\r\n",
        "      if '.jpg' in filename:\r\n",
        "        file_id = int(filename.split('_')[1].split('.')[0]) -1\r\n",
        "        file_class = str(labels[file_id]) + '_label'\r\n",
        "        #print(np.random.random())\r\n",
        "      try:\r\n",
        "        p = np.random.random()\r\n",
        "        #print(p)\r\n",
        "        #print(folderName+ '/'+ filename,validfoder+ '/'+file_class +'/'+ filename)\r\n",
        "        \r\n",
        "        if p < split:\r\n",
        "          # 放入验证集\r\n",
        "          #print(folderName+ '/'+ filename,validfoder+ '/'+file_class +'/'+ filename)\r\n",
        "          shutil.move(folderName+ '/'+ filename,validfoder+ '/'+file_class +'/'+ filename)\r\n",
        "          \r\n",
        "          pass\r\n",
        "        else:\r\n",
        "          # 放入训练集\r\n",
        "          shutil.move(folderName+ '/'+ filename,trainfolder+ '/'+file_class +'/'+ filename)\r\n",
        "          pass\r\n",
        "        \r\n",
        "        pass \r\n",
        "      except:\r\n",
        "        print('io erro')\r\n",
        "        pass\r\n",
        "mkdir_for_keras('train')\r\n",
        "mkdir_for_keras('valid')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxpqsNzxfayK"
      },
      "source": [
        "move2dir('jpg','train','valid',labels)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj8YVZedfhSH"
      },
      "source": [
        "def get_datasets(data_dir, input_size, is_train_data):\r\n",
        "    if(is_train_data):\r\n",
        "        images = datasets.ImageFolder(os.path.join(data_dir, \"train\"),\r\n",
        "                                      transforms.Compose([\r\n",
        "                                          transforms.RandomResizedCrop(input_size),\r\n",
        "                                          transforms.RandomHorizontalFlip(),\r\n",
        "                                          transforms.ToTensor()\r\n",
        "                                          ]))\r\n",
        "    else:\r\n",
        "        images = datasets.ImageFolder(os.path.join(data_dir, \"valid\"),\r\n",
        "                                      transforms.Compose([\r\n",
        "                                          transforms.Resize(input_size),\r\n",
        "                                          transforms.CenterCrop(input_size),\r\n",
        "                                          transforms.ToTensor()\r\n",
        "                                          ]))\r\n",
        "    return images\r\n",
        "train_images = get_datasets('.', input_size, is_train_data=True)\r\n",
        "test_images = get_datasets('.', input_size, is_train_data=False)\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(train_images, batch_size=batch_size, shuffle=True)\r\n",
        "test_loader = torch.utils.data.DataLoader(test_images, batch_size=batch_size)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0wsQ1jLe9rZ"
      },
      "source": [
        "#%%\r\n",
        "\r\n",
        "# model_ft = models.vgg19_bn(pretrained=True)\r\n",
        "# num_ftrs = model_ft.classifier[0].in_features\r\n",
        "\r\n",
        "model_ft = models.wide_resnet101_2(pretrained=True)\r\n",
        "num_ftrs = model_ft.fc.in_features\r\n",
        "\r\n",
        "half_in_size = round(num_ftrs/2)\r\n",
        "layer_width = 20 #Small for Resnet, large for VGG\r\n",
        "Num_class=10\r\n",
        "\r\n",
        "class SpinalNet_ResNet(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(SpinalNet_ResNet, self).__init__()\r\n",
        "        \r\n",
        "        self.fc_spinal_layer1 = nn.Sequential(\r\n",
        "            #nn.Dropout(p = 0.5), \r\n",
        "            nn.Linear(half_in_size, layer_width),\r\n",
        "            #nn.BatchNorm1d(layer_width), \r\n",
        "            nn.ReLU(inplace=True),)\r\n",
        "        self.fc_spinal_layer2 = nn.Sequential(\r\n",
        "            #nn.Dropout(p = 0.5), \r\n",
        "            nn.Linear(half_in_size+layer_width, layer_width),\r\n",
        "            #nn.BatchNorm1d(layer_width), \r\n",
        "            nn.ReLU(inplace=True),)\r\n",
        "        self.fc_spinal_layer3 = nn.Sequential(\r\n",
        "            #nn.Dropout(p = 0.5), \r\n",
        "            nn.Linear(half_in_size+layer_width, layer_width),\r\n",
        "            #nn.BatchNorm1d(layer_width), \r\n",
        "            nn.ReLU(inplace=True),)\r\n",
        "        self.fc_spinal_layer4 = nn.Sequential(\r\n",
        "            #nn.Dropout(p = 0.5), \r\n",
        "            nn.Linear(half_in_size+layer_width, layer_width),\r\n",
        "            #nn.BatchNorm1d(layer_width), \r\n",
        "            nn.ReLU(inplace=True),)\r\n",
        "        self.fc_out = nn.Sequential(\r\n",
        "            #nn.Dropout(p = 0.5), \r\n",
        "            nn.Linear(layer_width*4, num_classes),)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        x1 = self.fc_spinal_layer1(x[:, 0:half_in_size])\r\n",
        "        x2 = self.fc_spinal_layer2(torch.cat([ x[:,half_in_size:2*half_in_size], x1], dim=1))\r\n",
        "        x3 = self.fc_spinal_layer3(torch.cat([ x[:,0:half_in_size], x2], dim=1))\r\n",
        "        x4 = self.fc_spinal_layer4(torch.cat([ x[:,half_in_size:2*half_in_size], x3], dim=1))\r\n",
        "        \r\n",
        "        \r\n",
        "        x = torch.cat([x1, x2], dim=1)\r\n",
        "        x = torch.cat([x, x3], dim=1)\r\n",
        "        x = torch.cat([x, x4], dim=1)\r\n",
        "\r\n",
        "        \r\n",
        "        x = self.fc_out(x)\r\n",
        "        return x\r\n",
        "    \r\n",
        "class SpinalNet_VGG(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(SpinalNet_VGG, self).__init__()\r\n",
        "        \r\n",
        "        self.fc_spinal_layer1 = nn.Sequential(\r\n",
        "            nn.Dropout(p = 0.5), nn.Linear(half_in_size, layer_width),\r\n",
        "            nn.BatchNorm1d(layer_width), nn.ReLU(inplace=True),)\r\n",
        "        self.fc_spinal_layer2 = nn.Sequential(\r\n",
        "            nn.Dropout(p = 0.5), \r\n",
        "            nn.Linear(half_in_size+layer_width, layer_width),\r\n",
        "            nn.BatchNorm1d(layer_width), \r\n",
        "            nn.ReLU(inplace=True),)\r\n",
        "        self.fc_spinal_layer3 = nn.Sequential(\r\n",
        "            nn.Dropout(p = 0.5), \r\n",
        "            nn.Linear(half_in_size+layer_width, layer_width),\r\n",
        "            nn.BatchNorm1d(layer_width), \r\n",
        "            nn.ReLU(inplace=True),)\r\n",
        "        self.fc_spinal_layer4 = nn.Sequential(\r\n",
        "            nn.Dropout(p = 0.5), \r\n",
        "            nn.Linear(half_in_size+layer_width, layer_width),\r\n",
        "            nn.BatchNorm1d(layer_width), \r\n",
        "            nn.ReLU(inplace=True),)\r\n",
        "        self.fc_out = nn.Sequential(\r\n",
        "            nn.Dropout(p = 0.5), \r\n",
        "            nn.Linear(layer_width*4, num_classes),)        \r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x1 = self.fc_spinal_layer1(x[:, 0:half_in_size])\r\n",
        "        x2 = self.fc_spinal_layer2(torch.cat([ x[:,half_in_size:2*half_in_size], x1], dim=1))\r\n",
        "        x3 = self.fc_spinal_layer3(torch.cat([ x[:,0:half_in_size], x2], dim=1))\r\n",
        "        x4 = self.fc_spinal_layer4(torch.cat([ x[:,half_in_size:2*half_in_size], x3], dim=1))\r\n",
        "        \r\n",
        "        \r\n",
        "        x = torch.cat([x1, x2], dim=1)\r\n",
        "        x = torch.cat([x, x3], dim=1)\r\n",
        "        x = torch.cat([x, x4], dim=1)\r\n",
        "\r\n",
        "        \r\n",
        "        x = self.fc_out(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "VGG_fc = nn.Sequential(\r\n",
        "            nn.Linear(512, 4096),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Dropout(),\r\n",
        "            nn.Linear(4096, 4096),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Dropout(),\r\n",
        "            nn.Linear(4096, Num_class)\r\n",
        "        )\r\n",
        "\r\n",
        "\r\n",
        "'''\r\n",
        "Changing the fully connected layer to SpinalNet or VGG or ResNet\r\n",
        "'''\r\n",
        "\r\n",
        "#model_ft.fc = nn.Linear(num_ftrs, 10)\r\n",
        "model_ft.fc = SpinalNet_ResNet() #SpinalNet_VGG"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWfAxol6fnp7"
      },
      "source": [
        "#验证\r\n",
        "def test(model, test_loader, loss_func):\r\n",
        "    model.eval()\r\n",
        "    loss_val = 0.0\r\n",
        "    corrects = 0.0\r\n",
        "    for images, labels in test_loader:\r\n",
        "        images = images.to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "        with torch.no_grad():\r\n",
        "            outputs = model(images)\r\n",
        "            loss = loss_func(outputs, labels)\r\n",
        "            \r\n",
        "        _, predicts = torch.max(outputs, 1)\r\n",
        "        \r\n",
        "        loss_val += loss.item() * images.size(0)\r\n",
        "        corrects += torch.sum(predicts.view(-1) == labels.view(-1)).item()\r\n",
        "        \r\n",
        "    test_loss = loss_val / len(test_loader.dataset)\r\n",
        "    test_acc = corrects / len(test_loader.dataset)\r\n",
        "    \r\n",
        "    print(\"Test Loss: {}, Test Acc: {}\".format(test_loss, test_acc))\r\n",
        "        \r\n",
        "    return test_acc\r\n",
        "            \r\n",
        "#训练\r\n",
        "def train(model, train_loader, test_loader, loss_func, optimizer, num_epochs):\r\n",
        "    #初始化最好的验证准确率\r\n",
        "    best_val_acc = 0.0\r\n",
        "    #初始化最好的模型参数，采用deepcopy为防止优化过程中修改到best_model_params\r\n",
        "    best_model_params = copy.deepcopy(model.state_dict())\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        model.train()\r\n",
        "        loss_val = 0.0\r\n",
        "        corrects = 0.0\r\n",
        "        for images, labels in train_loader:\r\n",
        "            images = images.to(device)\r\n",
        "            labels = labels.to(device)\r\n",
        "            \r\n",
        "            outputs = model(images)\r\n",
        "            loss = loss_func(outputs, labels)\r\n",
        "            \r\n",
        "            #找出输出的最大概率所在的为\r\n",
        "            #二分类中：如果第一个样本输出的最大值出现在第0为，则其预测值为0\r\n",
        "            _, predicts = torch.max(outputs, 1) \r\n",
        "            \r\n",
        "            optimizer.zero_grad()\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            \r\n",
        "            #loss.item()为一个batch的平均loss的值\r\n",
        "            #images.size(0)为当前batch中有多少样本量\r\n",
        "            #loss.item() * images.size(0)表示一个batch的总loss值\r\n",
        "            loss_val += loss.item() * images.size(0)\r\n",
        "            \r\n",
        "            #view(-1)表示将tensor resize成一个维度为[batch_size]的tensor\r\n",
        "            #计算预测值与标签值相同的数量\r\n",
        "            corrects += torch.sum(predicts.view(-1) == labels.view(-1)).item()\r\n",
        "        \r\n",
        "        #计算每个epoch的平均loss\r\n",
        "        train_loss = loss_val / len(train_loader.dataset)\r\n",
        "        #预测准确的数量除以总的样本量即为准确率\r\n",
        "        train_acc = corrects / len(train_loader.dataset)\r\n",
        "        \r\n",
        "        print(\"Train Loss: {}, Train Acc: {}\".format(train_loss, train_acc))\r\n",
        "        \r\n",
        "        #调用测试\r\n",
        "        test_acc = test(model, test_loader, loss_func)\r\n",
        "        #根据测试准确率跟新最佳模型的参数\r\n",
        "        if(best_val_acc < test_acc):\r\n",
        "            best_val_acc = test_acc\r\n",
        "            best_model_params = copy.deepcopy(model.state_dict())\r\n",
        "    #将模型的最优参数载入模型    \r\n",
        "    model.load_state_dict(best_model_params)\r\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxOlQ4wufsny"
      },
      "source": [
        "def set_parameters_require_grad(model, is_fixed):\r\n",
        "    #默认parameter.requires_grad = True\r\n",
        "    #当采用固定预训练模型参数的方法进行训练时，将预训练模型的参数设置成不需要计算梯度\r\n",
        "    if(is_fixed):\r\n",
        "        for parameter in model.parameters():\r\n",
        "            parameter.requires_grad = False\r\n",
        "            \r\n",
        "def init_model(model_name, num_classes, is_fixed, use_pretrained):\r\n",
        "    if(model_name == 'resnet'):\r\n",
        "        #调用resnet模型，resnet18表示18层的resnet模型，\r\n",
        "        #pretrained=True表示需要加载预训练好的模型参数，pretrained=False表示不加载预训练好的模型参数\r\n",
        "        model = models.resnet18(pretrained=use_pretrained) #调用预训练的resnet18模型\r\n",
        "        #设置参数是否需要计算梯度\r\n",
        "        #is_fixed=True表示模型参数不需要跟新（不需要计算梯度）\r\n",
        "        #is_fixed=False表示模型参数需要fineturn（需要计算梯度）\r\n",
        "        set_parameters_require_grad(model, is_fixed)\r\n",
        "        \r\n",
        "        in_features = model.fc.in_features  #取出全连接层的输入特征维度\r\n",
        "        \r\n",
        "        #重新定义resnet18模型的全连接层,使其满足新的分类任务\r\n",
        "        #此时模型的全连接层默认需要计算梯度\r\n",
        "        model.fc = nn.Linear(in_features, num_classes) \r\n",
        "    if(model_name == 'mobilenet_v2'):\r\n",
        "        #调用resnet模型，resnet18表示18层的resnet模型，\r\n",
        "        #pretrained=True表示需要加载预训练好的模型参数，pretrained=False表示不加载预训练好的模型参数\r\n",
        "        model = models.mobilenet_v2(pretrained=use_pretrained) #调用预训练的resnet18模型\r\n",
        "        #设置参数是否需要计算梯度\r\n",
        "        #is_fixed=True表示模型参数不需要跟新（不需要计算梯度）\r\n",
        "        #is_fixed=False表示模型参数需要fineturn（需要计算梯度）\r\n",
        "        set_parameters_require_grad(model, is_fixed)\r\n",
        "        \r\n",
        "        model.classifier =  nn.Linear(1280, num_classes) \r\n",
        "        set_parameters_require_grad(model.classifier,False)\r\n",
        "    if model_name == 'SpinalNet_ResNet':\r\n",
        "      model = model_ft \r\n",
        "    return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dunj-syrgVqe"
      },
      "source": [
        "#获取需要更新的模型参数\r\n",
        "def get_require_updated_params(model, is_fixed):\r\n",
        "    if(is_fixed):\r\n",
        "        require_update_params = []\r\n",
        "        for param in model.parameters():\r\n",
        "            if(param.requires_grad):\r\n",
        "                require_update_params.append(param)\r\n",
        "        return require_update_params\r\n",
        "    else:\r\n",
        "        return model.parameters()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TmPRRYJgZfU",
        "outputId": "1daaf546-25f8-4ae6-e0c7-0f7b3d317f67"
      },
      "source": [
        "model = init_model(model_name, num_classes, is_fixed, use_pretrained)\r\n",
        "model = model.to(device)\r\n",
        "\r\n",
        "require_update_params = get_require_updated_params(model, is_fixed)\r\n",
        "\r\n",
        "#将需要跟新的参数放入优化器中进行优化\r\n",
        "optimizer = torch.optim.SGD(require_update_params, lr=lr, momentum=momentum)\r\n",
        "#交叉熵损失函数\r\n",
        "loss_func = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "if(is_train):\r\n",
        "    model = train(model, train_loader, test_loader, loss_func, optimizer, num_epochs)\r\n",
        "    torch.save(model.state_dict(),model_name+\".pt\")\r\n",
        "if(is_test):\r\n",
        "    model.load_state_dict(torch.load(model_name+\".pt\"))\r\n",
        "    acc = test(model, test_loader, loss_func)\r\n",
        "    print(\"Best Test Acc: {}\".format(acc))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Loss: 4.510962303412627, Train Acc: 0.06901300688599847\n",
            "Test Loss: 4.22810044513504, Test Acc: 0.12091898428053205\n",
            "Train Loss: 3.90528678106003, Train Acc: 0.17597551644988524\n",
            "Test Loss: 3.2894026065535815, Test Acc: 0.2877871825876663\n",
            "Train Loss: 3.0971706102527365, Train Acc: 0.35715378729915837\n",
            "Test Loss: 2.493050378928594, Test Acc: 0.4788391777509069\n",
            "Train Loss: 2.3950684112907825, Train Acc: 0.5063504208110176\n",
            "Test Loss: 1.8458844842760853, Test Acc: 0.5991535671100363\n",
            "Train Loss: 1.7895685260863183, Train Acc: 0.628309104820199\n",
            "Test Loss: 1.28578519518459, Test Acc: 0.7249093107617895\n",
            "Train Loss: 1.3074647190558062, Train Acc: 0.7358837031369548\n",
            "Test Loss: 0.802483280163434, Test Acc: 0.8355501813784765\n",
            "Train Loss: 0.9069301061258119, Train Acc: 0.8203519510328998\n",
            "Test Loss: 0.4771955367875935, Test Acc: 0.9105199516324063\n",
            "Train Loss: 0.6225042472746688, Train Acc: 0.8824789594491201\n",
            "Test Loss: 0.2982279660119399, Test Acc: 0.9437726723095526\n",
            "Train Loss: 0.4808617813402948, Train Acc: 0.9104820198928845\n",
            "Test Loss: 0.20289809592833974, Test Acc: 0.9649334945586457\n",
            "Train Loss: 0.3835473158748075, Train Acc: 0.9230298393267024\n",
            "Test Loss: 0.17477867540598205, Test Acc: 0.9685610640870617\n",
            "Train Loss: 0.31854802931914733, Train Acc: 0.9371078806426932\n",
            "Test Loss: 0.13991418700696767, Test Acc: 0.9721886336154776\n",
            "Train Loss: 0.2659926367745476, Train Acc: 0.9461361897475133\n",
            "Test Loss: 0.12111518623030315, Test Acc: 0.9758162031438936\n",
            "Train Loss: 0.23691774964697415, Train Acc: 0.9502677888293802\n",
            "Test Loss: 0.10458504749144436, Test Acc: 0.9770253929866989\n",
            "Train Loss: 0.21171906170903404, Train Acc: 0.9539403213465952\n",
            "Test Loss: 0.11033436633024855, Test Acc: 0.9727932285368803\n",
            "Train Loss: 0.20106086242080373, Train Acc: 0.957459831675593\n",
            "Test Loss: 0.08838765997047725, Test Acc: 0.9782345828295043\n",
            "Train Loss: 0.19851943182416243, Train Acc: 0.958530986993114\n",
            "Test Loss: 0.08914806998527396, Test Acc: 0.9800483675937122\n",
            "Train Loss: 0.1817215837270318, Train Acc: 0.9620504973221117\n",
            "Test Loss: 0.0844276490313831, Test Acc: 0.9842805320435308\n",
            "Train Loss: 0.16696102042734487, Train Acc: 0.9623565416985463\n",
            "Test Loss: 0.08867879593891273, Test Acc: 0.9818621523579202\n",
            "Train Loss: 0.1832859290175157, Train Acc: 0.9592960979342005\n",
            "Test Loss: 0.08199697741033665, Test Acc: 0.9830713422007256\n",
            "Train Loss: 0.15040071019254392, Train Acc: 0.9671002295332823\n",
            "Test Loss: 0.07170406260055816, Test Acc: 0.9836759371221282\n",
            "Test Loss: 0.0844276490313831, Test Acc: 0.9842805320435308\n",
            "Best Test Acc: 0.9842805320435308\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}